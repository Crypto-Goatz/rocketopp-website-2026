{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Crypto-Goatz/rocketopp-website-2026/blob/main/SEO_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "SEO NEURO ENGINE\n",
        "----------------\n",
        "1. Ingests Data (GSC, GA4, SERP)\n",
        "2. Calculates Opportunity Scores based on Weighted Factors\n",
        "3. Generates \"Perfect\" Content Briefs based on Strict Rules\n",
        "4. LEARNING LOOP: Compares previous actions to current results to auto-adjust weights.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import datetime as dt\n",
        "import random  # Used for simulation; replace with actual API data in prod\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION & GLOBAL WEIGHTS (Mutable)\n",
        "# ==========================================\n",
        "\n",
        "CONFIG = {\n",
        "    \"SITE_URL\": \"https://pittsburgh-ai-web.com\",\n",
        "    \"OUTPUT_DIR\": \"./seo_data\",\n",
        "    \"MIN_WORD_COUNT_PILLAR\": 2200,\n",
        "    \"MIN_WORD_COUNT_CLUSTER\": 1000,\n",
        "    \"KEYWORD_DENSITY_TARGET\": (0.006, 0.012) # 0.6% to 1.2%\n",
        "}\n",
        "\n",
        "# Initial Weights - The AI will adjust these over time\n",
        "WEIGHTS = {\n",
        "    \"W_IMPRESSIONS\": 0.35,\n",
        "    \"W_POSITION\": 0.25,\n",
        "    \"W_CTR_GAP\": 0.25,\n",
        "    \"W_CONVERSIONS\": 0.15,\n",
        "    \"VOLATILITY_PENALTY\": 0.12\n",
        "}\n",
        "\n",
        "EXPECTED_CTR_BY_POS = {\n",
        "    1: 0.32, 2: 0.20, 3: 0.13, 4: 0.09, 5: 0.07,\n",
        "    6: 0.05, 7: 0.04, 8: 0.03, 9: 0.025, 10: 0.02\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# DATA MODELS\n",
        "# ==========================================\n",
        "\n",
        "@dataclass\n",
        "class PageData:\n",
        "    url: str\n",
        "    primary_kw: str\n",
        "    clicks: int\n",
        "    impressions: int\n",
        "    ctr: float\n",
        "    position: float\n",
        "    conversions: int = 0\n",
        "    intent: str = \"mixed\" # local, transactional, informational\n",
        "    last_updated: str = \"\"\n",
        "\n",
        "@dataclass\n",
        "class ActionLog:\n",
        "    date: str\n",
        "    url: str\n",
        "    action_type: str # CTR_FIX, STRIKING_DISTANCE, etc.\n",
        "    original_metrics: Dict\n",
        "    weights_at_time: Dict\n",
        "\n",
        "# ==========================================\n",
        "# CORE ANALYTICS ENGINE\n",
        "# ==========================================\n",
        "\n",
        "class SEOEngine:\n",
        "    def __init__(self):\n",
        "        self.ensure_directories()\n",
        "        self.history_file = os.path.join(CONFIG[\"OUTPUT_DIR\"], \"learning_history.json\")\n",
        "        self.weights_file = os.path.join(CONFIG[\"OUTPUT_DIR\"], \"optimized_weights.json\")\n",
        "        self.load_weights()\n",
        "\n",
        "    def ensure_directories(self):\n",
        "        if not os.path.exists(CONFIG[\"OUTPUT_DIR\"]):\n",
        "            os.makedirs(CONFIG[\"OUTPUT_DIR\"])\n",
        "\n",
        "    def load_weights(self):\n",
        "        if os.path.exists(self.weights_file):\n",
        "            with open(self.weights_file, 'r') as f:\n",
        "                global WEIGHTS\n",
        "                WEIGHTS = json.load(f)\n",
        "                print(\"Loaded optimized weights from previous learning sessions.\")\n",
        "\n",
        "    def clamp(self, x, minimum, maximum):\n",
        "        return max(minimum, min(x, maximum))\n",
        "\n",
        "    def expected_ctr(self, position):\n",
        "        pos = int(round(position))\n",
        "        return EXPECTED_CTR_BY_POS.get(pos, 0.01)\n",
        "\n",
        "    def calculate_opportunity_score(self, page: PageData) -> float:\n",
        "        # Normalize Data\n",
        "        imp_score = self.clamp(math.log10(page.impressions + 1) / 5.0, 0, 1)\n",
        "        pos_score = self.clamp((50 - page.position) / 50.0, 0, 1)\n",
        "\n",
        "        exp_ctr = self.expected_ctr(page.position)\n",
        "        ctr_gap = self.clamp((exp_ctr - page.ctr) / exp_ctr, 0, 1) if exp_ctr > 0 else 0\n",
        "\n",
        "        conv_score = self.clamp(math.log10(page.conversions + 1) / 2.0, 0, 1)\n",
        "\n",
        "        # Apply Current Global Weights\n",
        "        score = (\n",
        "            WEIGHTS[\"W_IMPRESSIONS\"] * imp_score +\n",
        "            WEIGHTS[\"W_POSITION\"] * pos_score +\n",
        "            WEIGHTS[\"W_CTR_GAP\"] * ctr_gap +\n",
        "            WEIGHTS[\"W_CONVERSIONS\"] * conv_score\n",
        "        )\n",
        "        return round(score, 4)\n",
        "\n",
        "    def classify_bucket(self, page: PageData) -> str:\n",
        "        exp_ctr = self.expected_ctr(page.position)\n",
        "\n",
        "        # BUCKET 1: High Imp, Low CTR\n",
        "        if page.impressions > 200 and page.ctr < (exp_ctr * 0.7):\n",
        "            return \"CTR_FIX\"\n",
        "\n",
        "        # BUCKET 2: Striking Distance\n",
        "        if 4 <= page.position <= 15:\n",
        "            return \"STRIKING_DISTANCE\"\n",
        "\n",
        "        # BUCKET 3: Triage (Drop detection requires history, simplified here)\n",
        "        if page.position > 20 and page.impressions > 500:\n",
        "            return \"RELEVANCE_REBUILD\"\n",
        "\n",
        "        return \"MONITOR\"\n",
        "\n",
        "    # ==========================================\n",
        "    # THE LEARNING LOOP\n",
        "    # ==========================================\n",
        "\n",
        "    def run_learning_cycle(self, current_data: List[PageData]):\n",
        "        \"\"\"\n",
        "        Compare current metrics against the Action Log from 7-28 days ago.\n",
        "        If an action worked, reinforce the weights that prioritized it.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.history_file):\n",
        "            return\n",
        "\n",
        "        with open(self.history_file, 'r') as f:\n",
        "            history = json.load(f)\n",
        "\n",
        "        # Simple Learning Logic:\n",
        "        # 1. Find actions older than 7 days\n",
        "        # 2. Check if the URL's traffic/rank improved\n",
        "        # 3. If improved, nudge weights in favor of that action's characteristics\n",
        "\n",
        "        for entry in history:\n",
        "            # (Simulation of finding the matching current page)\n",
        "            curr_page = next((p for p in current_data if p.url == entry['url']), None)\n",
        "\n",
        "            if curr_page:\n",
        "                delta_traffic = curr_page.clicks - entry['original_metrics']['clicks']\n",
        "\n",
        "                if delta_traffic > 0:\n",
        "                    # REWARD: This action worked.\n",
        "                    # If it was a CTR fix, boost CTR weight slightly.\n",
        "                    if entry['action_type'] == \"CTR_FIX\":\n",
        "                        WEIGHTS[\"W_CTR_GAP\"] += 0.01\n",
        "                        print(f\"LEARNING: Boosting CTR Weight due to success on {entry['url']}\")\n",
        "                    elif entry['action_type'] == \"STRIKING_DISTANCE\":\n",
        "                        WEIGHTS[\"W_POSITION\"] += 0.01\n",
        "\n",
        "        # Re-normalize weights to sum to ~1.0\n",
        "        total = sum(WEIGHTS.values())\n",
        "        for k in WEIGHTS:\n",
        "            WEIGHTS[k] = round(WEIGHTS[k] / total, 3)\n",
        "\n",
        "        # Save new brain\n",
        "        with open(self.weights_file, 'w') as f:\n",
        "            json.dump(WEIGHTS, f, indent=2)\n",
        "\n",
        "    def log_actions(self, planned_actions: List[dict]):\n",
        "        \"\"\"Logs the specific actions taken today to be reviewed in the future.\"\"\"\n",
        "        existing_log = []\n",
        "        if os.path.exists(self.history_file):\n",
        "            with open(self.history_file, 'r') as f:\n",
        "                existing_log = json.load(f)\n",
        "\n",
        "        # Keep last 1000 actions\n",
        "        existing_log.extend(planned_actions)\n",
        "        existing_log = existing_log[-1000:]\n",
        "\n",
        "        with open(self.history_file, 'w') as f:\n",
        "            json.dump(existing_log, f, indent=2)\n",
        "\n",
        "# ==========================================\n",
        "# CONTENT & SCHEMA GENERATOR\n",
        "# ==========================================\n",
        "\n",
        "class ContentArchitect:\n",
        "    def generate_brief(self, page: PageData, bucket: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Generates the EXACT specs for the rewrite based on strict rules.\n",
        "        \"\"\"\n",
        "        is_pillar = \"pillar\" in page.url or page.impressions > 5000\n",
        "        target_words = 3200 if is_pillar else 1400\n",
        "\n",
        "        # Calculate Strict Densities\n",
        "        min_kw = int(target_words * CONFIG[\"KEYWORD_DENSITY_TARGET\"][0])\n",
        "        max_kw = int(target_words * CONFIG[\"KEYWORD_DENSITY_TARGET\"][1])\n",
        "\n",
        "        brief = {\n",
        "            \"meta_strategy\": {\n",
        "                \"title\": f\"Create 3 variations. Must include '{page.primary_kw}' near front. Use brackets for CTR e.g. [Year Update]\",\n",
        "                \"h1\": f\"Exact match: {page.primary_kw}\",\n",
        "                \"slug\": page.url.split('/')[-1]\n",
        "            },\n",
        "            \"structural_rules\": {\n",
        "                \"target_word_count\": target_words,\n",
        "                \"reading_level\": \"Grade 8\",\n",
        "                \"paragraph_limit\": \"85 words max\",\n",
        "                \"sentence_limit\": \"20 words avg\"\n",
        "            },\n",
        "            \"keyword_engineering\": {\n",
        "                \"primary\": page.primary_kw,\n",
        "                \"density_target\": \"0.6% - 1.2%\",\n",
        "                \"exact_count_range\": [min_kw, max_kw],\n",
        "                \"mandatory_placements\": [\n",
        "                    \"First 100 words\",\n",
        "                    \"One H2 exact match\",\n",
        "                    \"Last 120 words\"\n",
        "                ]\n",
        "            },\n",
        "            \"schema_stack\": self.get_schema_strategy(page.intent, page.url)\n",
        "        }\n",
        "\n",
        "        # Bucket specific instructions\n",
        "        if bucket == \"CTR_FIX\":\n",
        "            brief[\"priority_action\"] = \"REWRITE_META_AND_INTRO\"\n",
        "            brief[\"specific_instruction\"] = \"The content ranks but doesn't get clicks. Rewrite the Title Tag and the first <p> to be a 'Hook' or 'Direct Answer'.\"\n",
        "        elif bucket == \"STRIKING_DISTANCE\":\n",
        "            brief[\"priority_action\"] = \"EXPAND_DEPTH\"\n",
        "            brief[\"specific_instruction\"] = \"Rank is 4-15. Add 2 new H2s covering 'Benefits' and 'Case Studies'. Add internal links to 3 related clusters.\"\n",
        "\n",
        "        return brief\n",
        "\n",
        "    def get_schema_strategy(self, intent, url):\n",
        "        stack = [\"Organization\", \"WebSite\", \"BreadcrumbList\"]\n",
        "\n",
        "        if \"pittsburgh\" in url or intent == \"local\":\n",
        "            stack.extend([\"LocalBusiness\", \"Service\", \"AreaServed\"])\n",
        "        elif intent == \"transactional\":\n",
        "            stack.extend([\"Service\", \"FAQPage\", \"Product\"])\n",
        "        else:\n",
        "            stack.extend([\"Article\", \"FAQPage\", \"Person (Author)\"])\n",
        "\n",
        "        return stack\n",
        "\n",
        "# ==========================================\n",
        "# MAIN EXECUTION (MOCKED FOR DEMO)\n",
        "# ==========================================\n",
        "\n",
        "def run_simulation():\n",
        "    # 1. Initialize Engines\n",
        "    brain = SEOEngine()\n",
        "    architect = ContentArchitect()\n",
        "\n",
        "    # 2. Mock Data Ingestion (Replace with API calls to GSC/GA4)\n",
        "    # Simulating a mix of performing and underperforming pages\n",
        "    mock_pages = [\n",
        "        PageData(\"site.com/ai-business\", \"ai for business\", 120, 5000, 0.024, 6.2, 5, \"informational\"),\n",
        "        PageData(\"site.com/custom-crm\", \"custom crm pittsburgh\", 15, 800, 0.018, 12.5, 2, \"local\"),\n",
        "        PageData(\"site.com/seo-guide\", \"seo guide 2024\", 800, 15000, 0.05, 3.1, 10, \"informational\"),\n",
        "        PageData(\"site.com/web-dev\", \"web development\", 50, 4000, 0.012, 8.4, 1, \"transactional\"),\n",
        "        PageData(\"site.com/old-post\", \"marketing tips\", 5, 200, 0.025, 22.0, 0, \"informational\"),\n",
        "    ]\n",
        "\n",
        "    # 3. Run Learning Loop (Adjust weights based on past history - simulated)\n",
        "    brain.run_learning_cycle(mock_pages)\n",
        "\n",
        "    # 4. Generate Daily Plan\n",
        "    daily_plan = []\n",
        "    actions_to_log = []\n",
        "\n",
        "    for page in mock_pages:\n",
        "        score = brain.calculate_opportunity_score(page)\n",
        "        bucket = brain.classify_bucket(page)\n",
        "\n",
        "        if bucket != \"MONITOR\":\n",
        "            brief = architect.generate_brief(page, bucket)\n",
        "\n",
        "            plan_item = {\n",
        "                \"url\": page.url,\n",
        "                \"score\": score,\n",
        "                \"bucket\": bucket,\n",
        "                \"metrics\": asdict(page),\n",
        "                \"brief\": brief\n",
        "            }\n",
        "            daily_plan.append(plan_item)\n",
        "\n",
        "            actions_to_log.append({\n",
        "                \"date\": dt.date.today().isoformat(),\n",
        "                \"url\": page.url,\n",
        "                \"action_type\": bucket,\n",
        "                \"original_metrics\": asdict(page),\n",
        "                \"weights_at_time\": WEIGHTS.copy()\n",
        "            })\n",
        "\n",
        "    # Sort by Opportunity Score\n",
        "    daily_plan.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    # 5. Save Action Log for future learning\n",
        "    brain.log_actions(actions_to_log)\n",
        "\n",
        "    # 6. Output Final JSON for Dashboard\n",
        "    output = {\n",
        "        \"date\": dt.date.today().isoformat(),\n",
        "        \"active_weights\": WEIGHTS,\n",
        "        \"tasks\": daily_plan\n",
        "    }\n",
        "\n",
        "    print(json.dumps(output, indent=2))\n",
        "    return output\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_simulation()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "dfSlBlqE6YOk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}